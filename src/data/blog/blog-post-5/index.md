---
category: 'blog'
cover: './cover.jpg'
title: 'Blog Post 5'
description: 'Mauris neque libero, aliquet vel mollis nec, euismod sed tellus. Mauris convallis dictum elit id volutpat.'
date: '2020-02-03'
tags: ['Mobile', 'React Native']
published: true
---

<!DOCTYPE html>
<html lang="zh-Hans-CN"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=Edge"/><link rel="stylesheet" type="text/css" href="css/modern-norm.min.css"/><link rel="stylesheet" type="text/css" href="css/prism.min.css"/><link rel="stylesheet" type="text/css" href="css/katex.min.css"/><link rel="stylesheet" type="text/css" href="css/wolai.css"/><title>Pytorch学习 - wolai 笔记</title><link rel="shortcut icon" href="data:image/svg+xml,%3Csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 800 800&apos;%3E%3Cdefs%3E%3Cstyle%3E.cls-1%7Bfill:%23fff;%7D%3C/style%3E%3C/defs%3E%3Cg%3E%3Cpath class=&apos;cls-1&apos; d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Z&apos;/%3E%3Cpath d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Zm4.72,88.9H185.2L172.42,89c-32.78.62-43.68,3.24-54.71,9.14a45.84,45.84,0,0,0-19.54,19.54c-6.61,12.36-9.11,24.55-9.27,67.49V614.8L89,627.58c.62,32.78,3.24,43.68,9.14,54.71a45.84,45.84,0,0,0,19.54,19.54c12.36,6.61,24.55,9.11,67.49,9.27H610.08c46.79,0,59.41-2.44,72.21-9.28a45.84,45.84,0,0,0,19.54-19.54c6.61-12.36,9.11-24.55,9.27-67.49V189.92c0-46.79-2.44-59.41-9.28-72.21a45.84,45.84,0,0,0-19.54-19.54C669.93,91.56,657.74,89.06,614.8,88.9ZM233.33,493.33A73.34,73.34,0,1,1,160,566.67,73.35,73.35,0,0,1,233.33,493.33Z&apos;/%3E%3C/g%3E%3C/svg%3E"></link></head><body><header><div class="image"></div><div class="title"><div class="banner"><div class="icon"></div></div><div data-title="Pytorch学习" class="main-title"></div></div></header><article><h1 id="nGCx4e24eXH71MphgnLen" class="wolai-block"><span class="inline-wrap">前言（tricks）</span></h1><h2 id="exGJi4ANFFj2UZj86H1JeK" class="wolai-block"><span class="inline-wrap">批量处理的技巧</span></h2><div id="ugvaDntxCZLBDmoBv5AemS" class="wolai-block wolai-text"><div><span class="inline-wrap">在深度学习中，由于源数据都比较大，所以通常需要用到批处理。如利用批量来计算梯度的随机梯度法（SGD）就是一个典型应用。深度学习的计算一般比较复杂，并且数据量一般比较大，如果一次处理整个数据，较大概率会出现资源瓶颈。为了更有效地计算，一般将整个数据集分批次处理。与处理整个数据集相反的另一个极端是每次只处理一条记录，这种方法也不科学，一次处理一条记录无法充分发挥<span class="jill"></span>GPU、Numpy<span class="jill"></span>的平行处理优势。因此，在实际使用中往往采用批量处理（Mini-Batch）的方法。</span></div></div><div id="6qHC51uSa3wyfoRxNyYK6a" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="u8XBHFvmdLocapug6Fe2Ms" class="wolai-block wolai-text"><div><span class="inline-wrap">如何把大数据拆分成多个批次呢？可采用如下步骤：</span></div></div><div id="dxap3by4wPi3PnN9gsC2C8" class="wolai-block wolai-text"><div><span class="inline-wrap">1）得到数据集</span></div></div><div id="9MzstjUzRQMk8KQCft18zf" class="wolai-block wolai-text"><div><span class="inline-wrap">2）随机打乱数据</span></div></div><div id="dRJWvQYsMmZomtAUZejxkG" class="wolai-block wolai-text"><div><span class="inline-wrap">3）定义批大小</span></div></div><div id="eXHowb4H5oJAEZFy4BNknm" class="wolai-block wolai-text"><div><span class="inline-wrap">4）批处理数据集</span></div></div><div id="2ZpJnLBSJCGHkXo6ExXS76" class="wolai-block wolai-text"><div><span class="inline-wrap">下面我们通过一个示例来具体说明：</span></div></div><code-block id="qiKGc3UtGAajfSkGfWaaEe" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment">#生成10000个形状为2X3的矩阵</span>
data_train <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment">#这是一个3维矩阵，第1个维度为样本数，后两个是数据形状</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>data_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment">#(10000,2,3)</span>
<span class="token comment">#打乱这10000条数据</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>data_train<span class="token punctuation">)</span>
<span class="token comment">#定义批量大小</span>
batch_size<span class="token operator">=</span><span class="token number">100</span>
<span class="token comment">#进行批处理</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>data_train<span class="token punctuation">)</span><span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x_batch_sum<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>data_train<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"第{}批次,该批次的数据之和:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span>x_batch_sum<span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre></div></code-block><h1 id="oyyjN42H1Vu2KrBuxQAFpw" class="wolai-block"><span class="inline-wrap">PyTorch<span class="jill"></span>基础</span></h1><blockquote id="9Sg7Nxbqc9La6LirySJwTX" class="wolai-block"><span class="inline-wrap">PyTorch<span class="jill"></span>采用<span class="jill"></span>Python<span class="jill"></span>语言接口来实现编程，非常容易上手。它就像带<span class="jill"></span>GPU<span class="jill"></span>的<span class="jill"></span>Numpy，与<span class="jill"></span>Python<span class="jill"></span>一样都属于动态框架。PyTorch<span class="jill"></span>继承了<span class="jill"></span>Torch<span class="jill"></span>灵活、动态的编程环境和用户友好的界面，支持以快速和灵活的方式构建动态神经网络，还允许在训练过程中快速更改代码而不妨碍其性能，支持动态图形等尖端<span class="jill"></span>AI<span class="jill"></span>模型的能力，是快速实验的理想选择。</span></blockquote><div id="qv7EhfvzsQeSL3eWX75aNA" class="wolai-block wolai-text"><div><span class="inline-wrap">PyTorch<span class="jill"></span>是一个建立在<span class="jill"></span>Torch<span class="jill"></span>库之上的<span class="jill"></span>Python<span class="jill"></span>包，旨在加速深度学习应用。它提供一种类似<span class="jill"></span>Numpy<span class="jill"></span>的抽象方法来表征张量（或多维数组），可以利用<span class="jill"></span>GPU<span class="jill"></span>来加速训练。由于<span class="jill"></span>PyTorch<span class="jill"></span>采用了动态计算图（Dynamic Computational Graph）结构，且基于<span class="jill"></span>tape<span class="jill"></span>的<span class="jill"></span>Autograd<span class="jill"></span>系统的深度神经网络。其他很多框架，比如<span class="jill"></span>TensorFlow（TensorFlow2.0<span class="jill"></span>也加入了动态网络的支持）、Caffe、CNTK、Theano<span class="jill"></span>等，采用静态计算图。使用<span class="jill"></span>PyTorch，通过一种称为<span class="jill"></span>Reverse-mode auto-differentiation（反向模式自动微分）的技术，可以零延迟或零成本地任意改变你的网络的行为。</span></div></div><div id="3i6Xi9FhYjPbJQ88zfnovP" class="wolai-block wolai-text"><div><span class="inline-wrap">PyTorch<span class="jill"></span>由<span class="jill"></span>4<span class="jill"></span>个主要的包组成：</span></div></div><div id="pQmhHZpvqXjXgwfUDvkRkD" class="wolai-block wolai-text"><div><span class="inline-wrap"><code>·torch</code></span><span class="inline-wrap">：类似于<span class="jill"></span>Numpy<span class="jill"></span>的通用数组库，可将张量类型转换为<span class="jill"></span>torch.cuda.TensorFloat，并在<span class="jill"></span>GPU<span class="jill"></span>上进行计算。</span></div></div><div id="2zBweSUeK1fWmEu1rmnoNc" class="wolai-block wolai-text"><div><span class="inline-wrap"><code>·torch.autograd</code></span><span class="inline-wrap">：用于构建计算图形并自动获取梯度的包。</span></div></div><div id="txmJ2x9qCqNHKnjZ3D1DYp" class="wolai-block wolai-text"><div><span class="inline-wrap"><code>·torch.nn</code></span><span class="inline-wrap">：具有共享层和损失函数的神经网络库。</span></div></div><div id="fhh37AfnPPH5hPdLc11XYW" class="wolai-block wolai-text"><div><span class="inline-wrap"><code>·torch.optim</code></span><span class="inline-wrap">：具有通用优化算法（如<span class="jill"></span>SGD、Adam<span class="jill"></span>等）的优化包</span></div></div><h2 id="bHjT29jK6YsrPJiCPYsW28" class="wolai-block"><span class="inline-wrap">Tensor<span class="jill"></span>概述</span></h2><blockquote id="idbivvwLza1EJzqxhvkyfX" class="wolai-block"><span class="inline-wrap">PyTorch<span class="jill"></span>的<span class="jill"></span>Tensor，它可以是零维（又称为标量或一个数）、一维、二维及多维的数组。Tensor<span class="jill"></span>自称为神经网络界的<span class="jill"></span>Numpy，它与<span class="jill"></span>Numpy<span class="jill"></span>相似，二者可以共享内存，且之间的转换非常方便和高效。不过它们也有不同之处，最大的区别就是<span class="jill"></span>Numpy<span class="jill"></span>会把<span class="jill"></span>ndarray<span class="jill"></span>放在<span class="jill"></span>CPU<span class="jill"></span>中进行加速运算，而由<span class="jill"></span>Torch<span class="jill"></span>产生的<span class="jill"></span>Tensor<span class="jill"></span>会放在<span class="jill"></span>GPU<span class="jill"></span>中进行加速运算（假设当前环境有<span class="jill"></span>GPU）</span></blockquote><div id="w9C59LcBp4pzyBeAsDqfPy" class="wolai-block wolai-text"><div><span class="inline-wrap">对<span class="jill"></span>Tensor<span class="jill"></span>的操作很多，从接口的角度来划分，可以分为两类：</span></div></div><ul class="wolai-block"><li id="eaJJMQVtKFLyv2GHkaaNMd"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">1）torch.function，如<span class="jill"></span>torch.sum、torch.add<span class="jill"></span>等；</span></li><li id="jUGvVbfXvgoVENTCdxN1dC"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">2）tensor.function，如<span class="jill"></span>tensor.view、tensor.add<span class="jill"></span>等。</span></li></ul><div id="6QVp9M3gCxgUWDY3NkFbYX" class="wolai-block wolai-text"><div><span class="inline-wrap">这些操作对大部分<span class="jill"></span>Tensor<span class="jill"></span>都是等价的，如<span class="jill"></span>torch.add(x,y)与<span class="jill"></span>x.add(y)等价。在实际使用时，可以根据个人爱好选择。如果从修改方式的角度来划分，可以分为以下两类：</span></div></div><ul class="wolai-block"><li id="sJmWLs51v86r3MnwNH2d16"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">1）不修改自身数据，如<span class="jill"></span>x.add(y)，x<span class="jill"></span>的数据不变，返回一个新的<span class="jill"></span>Tensor。</span></li><li id="x2JTaRdajLDNbtmEJqxyPP"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">2）修改自身数据，如<span class="jill"></span>x.add_(y)（运行符带下划线后缀），运算结果存在<span class="jill"></span>x<span class="jill"></span>中，x<span class="jill"></span>被修改。</span></li></ul><h3 id="kmuWW4xF4PESatBiuMmXKn" class="wolai-block"><span class="inline-wrap">创建<span class="jill"></span>Tensor</span></h3><div id="uJgp9Fe2XyDegD3khc9yoy" class="wolai-block wolai-text"><div><span class="inline-wrap">创建<span class="jill"></span>Tensor<span class="jill"></span>的方法有很多，可以从列表或<span class="jill"></span>ndarray<span class="jill"></span>等类型进行构建，也可根据指定的形状构建。常见的创建<span class="jill"></span>Tensor<span class="jill"></span>的方法可参考表：</span></div></div><div id="q4ZNoyDKsrSZAR9QLnPv9B" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image.png" style="width: 100%"/></figure></div><code-block id="k8yK2Sk2RkuurJArHgkq6U" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> torch
<span class="token comment">#根据list数据生成Tensor</span>
torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#根据指定形状生成Tensor</span>
torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment">#根据给定的Tensor的形状</span>
t<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#查看Tensor的形状</span>
t<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#shape与size()等价方式</span>
t<span class="token punctuation">.</span>shape
<span class="token comment">#根据已有形状创建Tensor</span>
torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>t<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></div></code-block><aside id="h1TkacGdnJQxE3p4qas5Up" class="bg-cultured wolai-block"><div data-symbol="📌" class="icon"></div><span class="inline-wrap">注意<span class="jill"></span>torch.Tensor<span class="jill"></span>与<span class="jill"></span>torch.tensor<span class="jill"></span>的几点区别：</span><ul class="wolai-block"><li id="etjsdRafLTZ3EwbF1iHMtq"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">torch.Tensor<span class="jill"></span>是<span class="jill"></span>torch.empty<span class="jill"></span>和<span class="jill"></span>torch.tensor<span class="jill"></span>之间的一种混合，但是，当传入数据时，torch.Tensor<span class="jill"></span>使用全局默认<span class="jill"></span>dtype（FloatTensor），而<span class="jill"></span>torch.tensor<span class="jill"></span>是从数据中推断数据类型。</span></li><li id="prSCmQniUaULXtQkwDWMwV"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">torch.tensor(1)返回一个固定值<span class="jill"></span>1，而<span class="jill"></span>torch.Tensor(1)返回一个大小为<span class="jill"></span>1<span class="jill"></span>的张量，它是随机初始化的值。</span></li></ul><code-block id="bq4evnU7fvKWueJWr1tTgz" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> torch
t1<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
t2<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t1的值{},t1的数据类型{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t1<span class="token punctuation">,</span>t1<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t2的值{},t2的数据类型{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t2<span class="token punctuation">,</span>t2<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre></div></code-block></aside><h3 id="cGSjutpjgSiD238dUoYkrb" class="wolai-block"><span class="inline-wrap">修改<span class="jill"></span>Tensor<span class="jill"></span>形状</span></h3><div id="dmvvLnz5gLH1rtA783Fzkw" class="wolai-block wolai-text"><div><span class="inline-wrap">在处理数据、构建网络层等过程中，经常需要了解<span class="jill"></span>Tensor<span class="jill"></span>的形状、修改<span class="jill"></span>Tensor<span class="jill"></span>的形状。与修改<span class="jill"></span>Numpy<span class="jill"></span>的形状类似，修改<span class="jill"></span>Tenor<span class="jill"></span>的形状也有很多类似函数，具体可参考表：</span></div></div><div id="bD8ZjiJqVayuPGUTkTNy9W" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_1.png" style="width: 100%"/></figure></div><code-block id="bFD13FuJyTq4EiX588uViT" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> torch
<span class="token comment">#生成一个形状为2x3的矩阵</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment">#查看矩阵的形状</span>
x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#结果为torch.Size([2, 3])</span>
<span class="token comment">#查看x的维度</span>
x<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#结果为2</span>
<span class="token comment">#把x变为3x2的矩阵</span>
x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token comment">#把x展平为1维向量</span>
y<span class="token operator">=</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
y<span class="token punctuation">.</span>shape
<span class="token comment">#添加一个维度</span>
z<span class="token operator">=</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>y<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment">#查看z的形状</span>
z<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#结果为torch.Size([1, 6])</span>
<span class="token comment">#计算Z的元素个数</span>
z<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#结果为6</span></pre></div></code-block><aside id="jYeFcr3hrRHiuG5pTfCRTc" class="bg-cultured wolai-block"><div data-symbol="📌" class="icon"></div><span class="inline-wrap">torch.view<span class="jill"></span>与<span class="jill"></span>torch.reshpae<span class="jill"></span>的异同
1）reshape()可以由<span class="jill"></span>torch.reshape()，也可由<span class="jill"></span>torch.Tensor.reshape()调用。但<span class="jill"></span>view()只可由<span class="jill"></span>torch.Tensor.view()来调用。
2）对于一个将要被<span class="jill"></span>view<span class="jill"></span>的<span class="jill"></span>Tensor，新的<span class="jill"></span>size<span class="jill"></span>必须与原来的<span class="jill"></span>size<span class="jill"></span>与<span class="jill"></span>stride<span class="jill"></span>兼容。否则，在<span class="jill"></span>view<span class="jill"></span>之前必须调用<span class="jill"></span>contiguous()方法。
3）同样也是返回与<span class="jill"></span>input<span class="jill"></span>数据量相同，但形状不同的<span class="jill"></span>Tensor。若满足<span class="jill"></span>view<span class="jill"></span>的条件，则不会<span class="jill"></span>copy，若不满足，则会<span class="jill"></span>copy。
4）如果你只想重塑张量，请使用<span class="jill"></span>torch.reshape。如果你还关注内存使用情况并希望确保两个张量共享相同的数据，请使用<span class="jill"></span>torch.view</span></aside><h3 id="hYAfRgYUHQFKwNdzTTG9ym" class="wolai-block"><span class="inline-wrap">索引操作</span></h3><div id="5JUroCLFn6qHxe7mxVKKsR" class="wolai-block wolai-text"><div><span class="inline-wrap">Tensor<span class="jill"></span>的索引操作与<span class="jill"></span>Numpy<span class="jill"></span>类似，一般情况下索引结果与源数据共享内存。从<span class="jill"></span>Tensor<span class="jill"></span>获取元素除了可以通过索引，也可以借助一些函数，常用的选择函数可参考表：</span></div></div><div id="aZLz7xgWfVuKePTeb26fwG" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_2.png" style="width: 100%"/></figure></div><code-block id="x3ZmkVhVhoTerepvXmQWmW" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> torch
<span class="token comment">#设置一个随机种子</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>
<span class="token comment">#生成一个形状为2x3的矩阵</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment">#根据索引获取第1行，所有数据</span>
x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
<span class="token comment">#获取最后一列数据</span>
x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token comment">#生成是否大于0的Byter张量</span>
mask<span class="token operator">=</span>x<span class="token operator">></span><span class="token number">0</span>
<span class="token comment">#获取大于0的值</span>
torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>x<span class="token punctuation">,</span>mask<span class="token punctuation">)</span>
<span class="token comment">#获取非0下标,即行，列索引</span>
torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
<span class="token comment">#获取指定索引对应的值,输出根据以下规则得到</span>
<span class="token comment">#out[i][j] = input[index[i][j]][j] # if dim == 0</span>
<span class="token comment">#out[i][j] = input[i][index[i][j]] # if dim == 1</span>
index<span class="token operator">=</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>index<span class="token punctuation">)</span>
index<span class="token operator">=</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
a<span class="token operator">=</span>torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>index<span class="token punctuation">)</span>
<span class="token comment">#把a的值返回到一个2x3的0矩阵中</span>
z<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
z<span class="token punctuation">.</span>scatter_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>index<span class="token punctuation">,</span>a<span class="token punctuation">)</span>
</pre></div></code-block><h3 id="bYumJkafsDhbXRHYMiSPfg" class="wolai-block"><span class="inline-wrap">逐元素操作</span></h3><div id="oEejTKER26ebPnMGmfqcmz" class="wolai-block wolai-text"><div><span class="inline-wrap">与<span class="jill"></span>Numpy<span class="jill"></span>一样，Tensor<span class="jill"></span>也有逐元素操作（Element-Wise），且操作内容相似，但使用函数可能不尽相同。大部分数学运算都属于逐元素操作，其输入与输出的形状相同。常见的逐元素操作可参考表：</span></div></div><div id="oEs1FFEWvjDaA9yb3NUEKx" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_3.png" style="width: 100%"/></figure></div><code-block id="mFZkXGEBrgDbrRs6D64bd5" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> torch
t <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
t1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
t2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment">#t+0.1*(t1/t2)</span>
torch<span class="token punctuation">.</span>addcdiv<span class="token punctuation">(</span>t<span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> t1<span class="token punctuation">,</span> t2<span class="token punctuation">)</span>
<span class="token comment">#计算sigmoid</span>
torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>t<span class="token punctuation">)</span>
<span class="token comment">#将t限制在[0,1]之间</span>
torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>t<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment">#t+2进行就地运算</span>
t<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
</pre></div></code-block><h3 id="oGKxB1UmJ7jdDo5uZG2UY9" class="wolai-block"><span class="inline-wrap">归并操作</span></h3><div id="oMpBQzLKWBKm6oNLSMNQhK" class="wolai-block wolai-text"><div><span class="inline-wrap">归并操作顾名思义，就是对输入进行归并或合计等操作，这类操作的输入输出形状一般并不相同，而且往往是输入大于输出形状。归并操作可以对整个<span class="jill"></span>Tensor，也可以沿着某个维度进行归并。常见的归并操作可参考表：</span></div></div><div id="vYCuq1pfu1SBTHBHBp564U" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_4.png" style="width: 100%"/></figure></div><aside id="kEQ8owp96uLoKFNCpDBcjr" class="bg-cultured wolai-block"><div data-symbol="📌" class="icon"></div><span class="inline-wrap">归并操作一般涉及一个<span class="jill"></span>dim<span class="jill"></span>参数，指定沿哪个维进行归并。另一个参数是<span class="jill"></span>keepdim，说明输出结果中是否保留维度<span class="jill"></span>1，缺省情况是<span class="jill"></span>False，即不保留。</span></aside><code-block id="hDg2nHgFHUXmLtMqAh1kvR" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> torch
<span class="token comment">#生成一个含6个数的向量</span>
a<span class="token operator">=</span>torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span>
<span class="token comment">#使用view方法，把a变为2x3矩阵</span>
a<span class="token operator">=</span>a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#沿y轴方向累加，即dim=0</span>
b<span class="token operator">=</span>a<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#b的形状为[3]</span>
<span class="token comment">#沿y轴方向累加，即dim=0,并保留含1的维度</span>
b<span class="token operator">=</span>a<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment">#b的形状为[1,3]</span>
</pre></div></code-block><h3 id="k7jmKqVVP6jsJHH4gQGVJ2" class="wolai-block"><span class="inline-wrap">比较操作</span></h3><div id="v8hsVNCdGSxi6vCgE1vR8F" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_5.png" style="width: 100%"/></figure></div><code-block id="wEwWUXDCwfTopRPSUSPMRg" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> torch
x<span class="token operator">=</span>torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment">#求所有元素的最大值</span>
torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment">#结果为10</span>
<span class="token comment">#求y轴方向的最大值</span>
torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#结果为[6,8,10]</span>
<span class="token comment">#求最大的2个元素</span>
torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#结果为[6,8,10],对应索引为tensor([[1, 1, 1]</span>
</pre></div></code-block><h3 id="8cLWNEWUR42XXMNUiZhotw" class="wolai-block"><span class="inline-wrap">矩阵操作</span></h3><div id="cQGWA8sMj9FCxRaxYKZUdH" class="wolai-block wolai-text"><div><span class="inline-wrap">机器学习和深度学习中存在大量的矩阵运算，常用的算法有两种：一种是逐元素乘法，另外一种是点积乘法。</span></div></div><div id="7RFqi7ksTsKc5p2Mvshnsa" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_6.png" style="width: 100%"/></figure></div><aside id="eDM8xhaSwWkNhcaho7Gtop" class="bg-cultured wolai-block"><div data-symbol="📌" class="icon"></div><span class="inline-wrap">1）Torch<span class="jill"></span>的<span class="jill"></span>dot<span class="jill"></span>与<span class="jill"></span>Numpy<span class="jill"></span>的<span class="jill"></span>dot<span class="jill"></span>有点不同，Torch<span class="jill"></span>中的<span class="jill"></span>dot<span class="jill"></span>是对两个为<span class="jill"></span>1D<span class="jill"></span>张量进行点积运算，Numpy<span class="jill"></span>中的<span class="jill"></span>dot<span class="jill"></span>无此限制。
2）mm<span class="jill"></span>是对<span class="jill"></span>2D<span class="jill"></span>的矩阵进行点积，bmm<span class="jill"></span>对含<span class="jill"></span>batch<span class="jill"></span>的<span class="jill"></span>3D<span class="jill"></span>进行点积运算。
3）转置运算会导致存储空间不连续，需要调用<span class="jill"></span>contiguous<span class="jill"></span>方法转为连续。</span></aside><h3 id="qrbAHRV1NU77qbrPKzMvQZ" class="wolai-block"><span class="inline-wrap">PyTorch<span class="jill"></span>和<span class="jill"></span>NumPy<span class="jill"></span>比较</span></h3><div id="emWcdGfbe6aVviYb1N3Uun" class="wolai-block wolai-text"><div><span class="inline-wrap">PyTorch<span class="jill"></span>与<span class="jill"></span>Numpy<span class="jill"></span>有很多类似的地方，并且有很多相同的操作函数名称，或虽然函数名称不同但含义相同；当然也有一些虽然函数名称相同，但含义不尽相同。有些很容易混淆，下面我们把一些主要的区别进行汇总</span></div></div><div id="6KYEbrbk8VZMLLRW3uSWw1" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_7.png" style="width: 100%"/></figure></div><h2 id="5ua15ggfnXEpzxUDg6CPkf" class="wolai-block"><span class="inline-wrap">Tensor<span class="jill"></span>与<span class="jill"></span>Autograd</span></h2><blockquote id="iS7Fe3ES5AmrLefFeJbXxY" class="wolai-block"><span class="inline-wrap">在神经网络中，一个重要内容就是进行参数学习，而参数学习离不开求导，那么<span class="jill"></span>PyTorch<span class="jill"></span>是如何进行求导的呢？
现在大部分深度学习架构都有自动求导的功能，PyTorch<span class="jill"></span>也不例外，torch.autograd<span class="jill"></span>包就是用来自动求导的。Autograd<span class="jill"></span>包为张量上所有的操作提供了自动求导功能，而<span class="jill"></span>torch.Tensor<span class="jill"></span>和<span class="jill"></span>torch.Function<span class="jill"></span>为<span class="jill"></span>Autograd<span class="jill"></span>的两个核心类，它们相互连接并生成一个有向非循环图。接下来我们先简单介绍<span class="jill"></span>Tensor<span class="jill"></span>如何实现自动求导，然后介绍计算图，最后用代码来实现这些功能</span></blockquote><h3 id="azbqXdDguUWG8GYJ5FZPSu" class="wolai-block"><span class="inline-wrap">自动求导要点</span></h3><div id="776zFmHf1xAbfQ3cSm3DHd" class="wolai-block wolai-text"><div><span class="inline-wrap">为实现对<span class="jill"></span>Tensor<span class="jill"></span>自动求导，需考虑如下事项：</span></div></div><ol class="wolai-block"><li id="eEXii3oi8hHvkdAC3sYSrZ"><div class="marker"></div><span class="inline-wrap">创建叶子节点（Leaf Node）的<span class="jill"></span>Tensor，使用</span><span class="inline-wrap"><code>requires_grad</code></span><span class="inline-wrap">参数指定是否记录对其的操作，以便之后利用<span class="jill"></span>backward()方法进行梯度求解。requires_grad<span class="jill"></span>参数的缺省值为<span class="jill"></span>False，如果要对其求导需设置为<span class="jill"></span>True，然后与之有依赖关系的节点会自动变为<span class="jill"></span>True。</span></li><li id="43YKwjt7BpHm6TzakpFZK"><div class="marker"></div><span class="inline-wrap">可利用<span class="jill"></span>requires_grad_()方法修改<span class="jill"></span>Tensor<span class="jill"></span>的<span class="jill"></span>requires_grad<span class="jill"></span>属性。可以调用</span><span class="inline-wrap"><code>.detach()</code></span><span class="inline-wrap">或</span><span class="inline-wrap"><code>with torch.no_grad()：</code></span><span class="inline-wrap">，将不再计算张量的梯度，跟踪张量的历史记录。</span><span class="inline-wrap"><b>这点在评估模型、测试模型阶段中常常用到。</b></span></li><li id="ns3KyvPdXwMZhvTZ2w6etG"><div class="marker"></div><span class="inline-wrap">通过运算创建的<span class="jill"></span>Tensor（即非叶子节点），会自动被赋予<span class="jill"></span>grad_fn<span class="jill"></span>属性。该属性表示梯度函数。叶子节点的<span class="jill"></span>grad_fn<span class="jill"></span>为<span class="jill"></span>None。</span></li><li id="79dCAcevDDNfaamoLRpV4h"><div class="marker"></div><span class="inline-wrap">最后得到的<span class="jill"></span>Tensor<span class="jill"></span>执行<span class="jill"></span>backward()函数，此时自动计算各变量的梯度，并将累加结果保存到<span class="jill"></span>grad<span class="jill"></span>属性中。计算完成后，非叶子节点的梯度自动释放。</span></li><li id="hG9Du29n2ZnPiMZcnx7qV1"><div class="marker"></div><span class="inline-wrap">backward()函数接收参数，该参数应和调用<span class="jill"></span>backward()函数的<span class="jill"></span>Tensor<span class="jill"></span>的维度相同，或者是可<span class="jill"></span>broadcast<span class="jill"></span>的维度。如果求导的<span class="jill"></span>Tensor<span class="jill"></span>为标量（即一个数字），则<span class="jill"></span>backward<span class="jill"></span>中的参数可省略。</span></li><li id="9791CxYwMMhHtVs2BksANF"><div class="marker"></div><span class="inline-wrap">反向传播的中间缓存会被清空，如果需</span><span class="inline-wrap"><b>要进行多次反向传播，需要指定<span class="jill"></span>backward<span class="jill"></span>中的参数<span class="jill"></span>retain_graph=True。多次反向传播时，梯度是累加的。</b></span><span class="inline-wrap">否则会将计算图清空。</span></li><li id="ddyczTdZz4fnvppFsbkpN1"><div class="marker"></div><span class="inline-wrap">非叶子节点的梯度<span class="jill"></span>backward<span class="jill"></span>调用后即被清空。</span></li><li id="sKPhGXTbyLkiP8raQMHmjn"><div class="marker"></div><span class="inline-wrap">可以通过用<span class="jill"></span>torch.no_grad()包裹代码块的形式来阻止<span class="jill"></span>autograd<span class="jill"></span>去跟踪那些标记</span></li></ol><div id="x95rpFoMgGcEMuePMH9HHr" class="wolai-block wolai-text"><div><span class="inline-wrap">为.requesgrad=True<span class="jill"></span>的张量的历史记录。这步在测试阶段经常使用。在整个过程中，PyTorch<span class="jill"></span>采用计算图的形式进行组织，该计算图为动态图，且在每次前向传播时，将重新构建。其他深度学习架构，如<span class="jill"></span>TensorFlow、Keras<span class="jill"></span>一般为静态图。接下来我们介绍计算图，用图的形式来描述就更直观了，该计算图为有向无环图（DAG）</span></div></div><div id="psqDUgkQGtdHEDZkdvgAMf" class="wolai-block wolai-text"><div><span class="inline-wrap">PyTorch<span class="jill"></span>调用<span class="jill"></span>backward()方法，将自动计算各节点的梯度，这是一个反向传播过程，这个过程可用图<span class="jill"></span>2-9<span class="jill"></span>表示。且在反向传播过程中，autograd<span class="jill"></span>沿着图<span class="jill"></span>2-10，从当前根节点<span class="jill"></span>z<span class="jill"></span>反向溯源，利用导数链式法则，计算所有叶子节点的梯度，其梯度值将累加到<span class="jill"></span>grad<span class="jill"></span>属性中。对非叶子节点的计算操作（或<span class="jill"></span>Function）记录在<span class="jill"></span>grad_fn<span class="jill"></span>属性中，叶子节点的<span class="jill"></span>grad_fn<span class="jill"></span>值为<span class="jill"></span>None。</span></div></div><div id="eTbGVZLT1H6D9D45VbuGxx" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_8.png" style="width: 220px"/></figure></div><h3 id="w98wyJDgGZnRopqMjP7fbL" class="wolai-block"><span class="inline-wrap">非标量反向传播</span></h3><div id="rmE1bmVVrMow6hKPEFuDM6" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>2.5.3<span class="jill"></span>节中介绍了当目标张量为标量时，可以调用<span class="jill"></span>backward()方法且无须传入参数。目标张量一般都是标量，如我们经常使用的损失值<span class="jill"></span>Loss，一般都是一个标量。但也有非标量的情况，后面将介绍的<span class="jill"></span>Deep Dream<span class="jill"></span>的目标值就是一个含多个元素的张量。那如何对非标量进行反向传播呢？PyTorch<span class="jill"></span>有个简单的规定，</span><span class="inline-wrap"><b>不让张量（Tensor）对张量求导，只允许标量对张量求导</b></span><span class="inline-wrap">，因此，如果目标张量对一个非标量调用<span class="jill"></span>ackward()，则需要传入一个<span class="jill"></span>gradient<span class="jill"></span>参数，该参数也是张量，而且需要与调用<span class="jill"></span>backward()的张量形状相同。那么为什么要传入一个张量<span class="jill"></span>gradient<span class="jill"></span>呢？</span></div></div><div id="ockFcJRQEymD9f9YHUm9Zo" class="wolai-block wolai-text"><div><span class="inline-wrap">传入这个参数就是为了把张量对张量的求导转换为标量对张量的求导。这有点拗口，我们举一个例子来说，假设目标值为</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">loss=(y_1,y_2,…,y_m)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">oss</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">，传入的参数为</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>v</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>v</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>v</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v=(v_1,v_2,…,v_m)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">，那么就可把对<span class="jill"></span>loss<span class="jill"></span>的求导，转换为对</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>∗</mo><msup><mi>v</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">loss*v^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">oss</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">标量的求导。即把原来 得到的雅可比矩阵（Jacobian）乘以张量</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">v^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">，便可得到我们需要的梯度矩阵。</span></div></div><code-block id="vWZha1LvtjqZFEZjdKEqWa" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre>backward<span class="token punctuation">(</span>gradient<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> create_graph<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</pre></div></code-block><div id="rdA4TTPMLi7VHU9iDRrXbQ" class="wolai-block wolai-text"><div><span class="inline-wrap">上面说的可能有点抽象，下面来通过一个实例进行说明。</span></div></div><div id="2Bhr8X6z7B4c6ALZByF9yb" class="wolai-block wolai-text"><div><span class="inline-wrap">1）定义叶子节点及计算节点。</span></div></div><code-block id="aAcY2xNQNSw4v1VH1cp4Lx" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> torch
<span class="token comment">#定义叶子节点张量x，形状为1x2</span>
x<span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment">#初始化Jacobian矩阵</span>
J<span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span> <span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token comment">#初始化目标张量，形状为1x2</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token comment">#定义y与x之间的映射关系：</span>
<span class="token comment">#y1=x12+3*x2，y2=x22+2*x1</span>
y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">3</span> <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">0</span> <span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>
y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span></pre></div></code-block><div id="nj7R4T3Ky4FD1h4DL5VEgb" class="wolai-block wolai-text"><div><span class="inline-wrap">2）手工计算<span class="jill"></span>y<span class="jill"></span>对<span class="jill"></span>x<span class="jill"></span>的梯度。</span></div></div><div id="3pmZdpqCorsZWMaCV1Pydm" class="wolai-block wolai-text"><div><span class="inline-wrap">我们先手工计算一下<span class="jill"></span>y<span class="jill"></span>对<span class="jill"></span>x<span class="jill"></span>的梯度，验证<span class="jill"></span>PyTorch<span class="jill"></span>的<span class="jill"></span>backward<span class="jill"></span>的结果是否正确。y<span class="jill"></span>对<span class="jill"></span>x<span class="jill"></span>的梯度是一个雅可比矩阵，我们可通过以下方法进行计算各项的值。假设<span class="jill"></span>x=(x1=2,x2=3), ，不难得到：</span></div></div><div id="qaX58d3BgMvuaopC1P8Nto" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>J</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mn>1</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mn>1</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>2</mn><msub><mi>x</mi><mn>1</mn></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>2</mn><msub><mi>x</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">J=\left(\begin{array}{ll}\frac{\partial y_{1}}{\partial x_{1}} &amp; \frac{\partial y_{1}}{\partial x_{2}} \\ \frac{\partial y_{2}}{\partial x_{1}} &amp; \frac{\partial y_{2}}{\partial x_{2}}\end{array}\right)=\left(\begin{array}{cc}2 x_{1} &amp; 3 \\ 2 &amp; 2 x_{2}\end{array}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6423em;"><span style="top:-3.6951em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9472em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.4611em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.3028em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9472em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.4611em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1423em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6423em;"><span style="top:-3.6951em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9472em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.4611em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.3028em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9472em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.4611em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1423em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span></span></span></span></span></div><div id="mYVNPzLnLT2wiB3HFmPU6T" class="wolai-block wolai-text"><div><span class="inline-wrap">3）调用<span class="jill"></span>backward<span class="jill"></span>来获取<span class="jill"></span>y<span class="jill"></span>对<span class="jill"></span>x<span class="jill"></span>的梯度。</span></div></div><code-block id="zjhdDJDfJoBSuhgE4nWvf" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token comment">#结果为tensor([[6., 9.]])</span></pre></div></code-block><div id="qUM73GvGgBdfsFtKQzEiwa" class="wolai-block wolai-text"><div><span class="inline-wrap">这个结果与我们手工运算的不符，显然这个结果是错误的，那错在哪里呢？这个结果的计算过程是：</span></div></div><div id="mNkRjFrTYb6u19rLNq4VNK" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>J</mi><mi mathvariant="normal">T</mi></msup><mo>⋅</mo><msup><mi>v</mi><mi mathvariant="normal">T</mi></msup><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>6</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>6</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>9</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">J^{\mathrm{T}} \cdot v^{\mathrm{T}}=\left(\begin{array}{ll}4 &amp; 2 \\ 3 &amp; 6\end{array}\right)\left(\begin{array}{l}1 \\ 1\end{array}\right)=\left(\begin{array}{l}6 \\ 9\end{array}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">T</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">T</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span></span></span></span></span></div><div id="mRKESZwKEKwudJUYFe2js9" class="wolai-block wolai-text"><div><span class="inline-wrap">由此可见，错在<span class="jill"></span>v<span class="jill"></span>的取值，通过这种方式得到的并不是<span class="jill"></span>y<span class="jill"></span>对<span class="jill"></span>x<span class="jill"></span>的梯度。这里我们可以分成两步计算。首先让<span class="jill"></span>v=(1,0)得到<span class="jill"></span>y1<span class="jill"></span>对<span class="jill"></span>x<span class="jill"></span>的梯度，然后使<span class="jill"></span>v=(0,1)，得到<span class="jill"></span>y2<span class="jill"></span>对<span class="jill"></span>x<span class="jill"></span>的梯度。这里因需要重复使用<span class="jill"></span>backward()，需要使参数<span class="jill"></span>retain_graph=True，具体代码如下：</span></div></div><code-block id="gLFMm59rM1gfJG9Bjs91K5" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token comment">#生成y1对x的梯度</span>
y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
J<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">=</span>x<span class="token punctuation">.</span>grad
<span class="token comment">#梯度是累加的，故需要对x的梯度清零</span>
x<span class="token punctuation">.</span>grad <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token comment">#生成y2对x的梯度</span>
y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
J<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">=</span>x<span class="token punctuation">.</span>grad
<span class="token comment">#显示jacobian矩阵的值</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>J<span class="token punctuation">)</span>

</pre></div></code-block><h1 id="wHae6fSDg9tgvuYPQBTpo7" class="wolai-block"><span class="inline-wrap">PyTorch<span class="jill"></span>神经网络工具箱</span></h1><h2 id="auAWErEMukjbGCf28kmYHJ" class="wolai-block"><span class="inline-wrap">神经网络核心组件</span></h2><div id="s2k284GWmyaXV25QkppJGf" class="wolai-block wolai-text"><div><span class="inline-wrap">神经网络看起来很复杂，节点很多，层数多，参数更多。但核心部分或组件不多，把这些组件确定后，这个神经网络基本就确定了。这些核心组件包括：</span></div></div><div id="uYDnxDZKqkMg1vUBQAmKpG" class="wolai-block wolai-text"><div><span class="inline-wrap">1）层：神经网络的基本结构，将输入张量转换为输出张量。</span></div></div><div id="g7F2b6k3sH1W4jSPHUosRi" class="wolai-block wolai-text"><div><span class="inline-wrap">2）模型：层构成的网络。</span></div></div><div id="mSXADzTWQuAaESrUff6Ejr" class="wolai-block wolai-text"><div><span class="inline-wrap">3）损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数。</span></div></div><div id="g5PEyhboypxGCyLP9zHC8c" class="wolai-block wolai-text"><div><span class="inline-wrap">4）优化器：如何使损失函数最小，这就涉及优化器。</span></div></div><div id="sB6EjnC5GEvYVGMSNHFhcg" class="wolai-block wolai-text"><div><span class="inline-wrap">当然这些核心组件不是独立的，它们之间，以及它们与神经网络其他组件之间有密切关系。</span></div></div><div id="64rtwUiSv34peTYBbJ1Z3M" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_9.png" style="width: 394px"/></figure></div><div id="pDdXPNbzG4ZePEw4dPT2A8" class="wolai-block wolai-text"><div><span class="inline-wrap">多个层链接在一起构成一个模型或网络，输入数据通过这个模型转换为预测值，然后损失函数把预测值与真实值进行比较，得到损失值（损失值可以是距离、概率值等），该损失值用于衡量预测值与目标结果的匹配或相似程度，优化器利用损失值更新权重参数，从而使损失值越来越小。这是一个循环过程，当损失值达到一个阀值或循环次数到达指定次数，循环结束。</span></div></div><div id="jA8yEY31G4CMytP4vPjhTu" class="wolai-block wolai-text"><div><span class="inline-wrap">接下来利用<span class="jill"></span>PyTorch<span class="jill"></span>的<span class="jill"></span>nn<span class="jill"></span>工具箱，构建一个神经网络实例。nn<span class="jill"></span>中对这些组件都有现成包或类，可以直接使用，非常方便</span></div></div><h1 id="eC37vaaMqxre37rnD1EQUx" class="wolai-block"><span class="inline-wrap">PyTorch<span class="jill"></span>数据处理工具箱</span></h1><blockquote id="7QvMh7hEsTjM7ptLuWV1u3" class="wolai-block"><span class="inline-wrap">Tensorboard<span class="jill"></span>是<span class="jill"></span>Google TensorFlow<span class="jill"></span>的可视化工具，它可以记录训练数据、评估数据、网络结构、图像等，并且可以在<span class="jill"></span>web<span class="jill"></span>上展示，对于观察神经网络训练的过程非常有帮助。PyTorch<span class="jill"></span>可以采用<span class="jill"></span>tensorboard_logger、visdom<span class="jill"></span>等可视化工具，但这些方法比较复杂或不够友好。为解决这一问题，人们推出了可用于<span class="jill"></span>PyTorch<span class="jill"></span>可视化的新的更强大的工具——tensorboardX</span></blockquote><h2 id="tpLGWksaYCmASXTSLG8vKf" class="wolai-block"><span class="inline-wrap">可视化工具</span></h2><h3 id="sE6am5UHv8TG9Ecdnq8vgr" class="wolai-block"><span class="inline-wrap">tensorboardX<span class="jill"></span>工具</span></h3><blockquote id="222RQfEHTZXA1Gxx2G6p7u" class="wolai-block"><span class="inline-wrap">tensorboardX<span class="jill"></span>功能很强大，支持<span class="jill"></span>scalar、image、figure、histogram、audio、text、graph、onnx_graph、embedding、pr_curve and videosummaries<span class="jill"></span>等可视化方式。</span></blockquote><div id="xna1jimW8jxRSZC9MsoQje" class="wolai-block wolai-text"><div><span class="inline-wrap">使用<span class="jill"></span>tensorboardX<span class="jill"></span>的一般步骤如下所示:</span></div></div><ul class="wolai-block"><li id="bH3hWxJ6cJ7vi392bt8sz6"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">导入<span class="jill"></span>tensorboardX，实例化<span class="jill"></span>SummaryWriter<span class="jill"></span>类，指明记录日志路径等信息。</span><code-block id="95wcCzDuepJeZH4hrcMSJL" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">from</span> tensorboardX <span class="token keyword">import</span> SummaryWriter
<span class="token comment">#实例化SummaryWriter，并指明日志存放路径。在当前目录没有logs目录将自动创建。</span>
writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span>log_dir<span class="token operator">=</span><span class="token string">'logs'</span><span class="token punctuation">)</span>
<span class="token comment">#调用实例</span>
writer<span class="token punctuation">.</span>add_xxx<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#关闭writer</span>
writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
</pre></div></code-block></li><li id="exTxyoghzWMT1afXNbBVaK"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">调用相应的<span class="jill"></span>API<span class="jill"></span>接口，接口一般格式为：</span><code-block id="tUzPPrps6AXmE51aTKjqWR" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre>add_xxx<span class="token punctuation">(</span>tag<span class="token operator">-</span>name<span class="token punctuation">,</span> <span class="token builtin">object</span><span class="token punctuation">,</span> iteration<span class="token operator">-</span>number<span class="token punctuation">)</span>
<span class="token comment">#即add_xxx(标签，记录的对象，迭代次数)</span></pre></div></code-block></li><li id="gLHZwrFcXqG9tAwe6yN3MX"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">启动<span class="jill"></span>tensorboard<span class="jill"></span>服务：</span><div id="3Pe9ZFxQvbpGVvV6xP1ARG" class="wolai-block wolai-text"><div><span class="inline-wrap">cd<span class="jill"></span>到<span class="jill"></span>logs<span class="jill"></span>目录所在的同级目录，在命令行输入如下命令，logdir<span class="jill"></span>等式右边可以是相对路径或绝对路径:</span></div></div><code-block id="u8eSMNrnjXhsUySFw9wJ8x" class="wolai-block"><div class="wolai-pre"><div data-lang="Bash" class="marker"></div><pre>tensorboard --logdir<span class="token operator">=</span>logs --port <span class="token number">6006</span>
<span class="token comment">#如果是Windows环境，要注意路径解析，如</span>
<span class="token comment">#tensorboard --logdir=r'D:\myboard\test\logs' --port 600</span>
</pre></div></code-block></li><li id="oxsJZoPSHwKwwFqmDKUUx5"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">web<span class="jill"></span>展示。</span><div id="c5H1CZxQumHWSBFEw6wdPj" class="wolai-block wolai-text"><div><span class="inline-wrap">在浏览器输入： </span><span class="inline-wrap"><a href="http://xn--IP-fr5c17cc4ew6ycgiln3b:6006"><span>http://服务器<span class="jill"></span>IP<span class="jill"></span>或名称:6006</span></a></span><span class="inline-wrap"> #如果是本机，服务器名称可以使用<span class="jill"></span>localhost</span></div></div></li></ul><h3 id="ofp3dU7QXSFJ292z2XtsJU" class="wolai-block"><span class="inline-wrap">用<span class="jill"></span>tensorboardX<span class="jill"></span>可视化神经网络（example）</span></h3><div id="55TbmB2Q8fWNRbnXNyjAJe" class="wolai-block wolai-text"><div><span class="inline-wrap">（1）导入需要的模块</span></div></div><code-block id="udxXCYUViySW9Qgr57MMxX" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> torchvision
<span class="token keyword">from</span> tensorboardX <span class="token keyword">import</span> SummaryWriter</pre></div></code-block><div id="b7BSJ9Lbmq6SX6Vck38Tsu" class="wolai-block wolai-text"><div><span class="inline-wrap">（2）构建神经网络</span></div></div><code-block id="dXem5JipM9jUzcq5cdZgdK" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>conv2_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">320</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>
  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">+</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2_drop<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">320</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> F<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">,</span> training<span class="token operator">=</span>self<span class="token punctuation">.</span>training<span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x
</pre></div></code-block><div id="6fr1sz3wz6Uf94hABpUrKa" class="wolai-block wolai-text"><div><span class="inline-wrap">（3）把模型保存为<span class="jill"></span>graph</span></div></div><code-block id="5VEinUrAgT85WSHjHeaKRu" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre><span class="token comment">#定义输入</span>
<span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>
<span class="token comment">#实例化神经网络</span>
model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#将model保存为graph</span>
<span class="token keyword">with</span> SummaryWriter<span class="token punctuation">(</span>log_dir<span class="token operator">=</span><span class="token string">'logs'</span><span class="token punctuation">,</span>comment<span class="token operator">=</span><span class="token string">'Net'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> w<span class="token punctuation">:</span>
  w<span class="token punctuation">.</span>add_graph<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span></pre></div></code-block><h3 id="pDPWpD9YR6grra5uQXbq2T" class="wolai-block"><span class="inline-wrap">用<span class="jill"></span>tensorboardX<span class="jill"></span>可视化损失值</span></h3><div id="u5W9rDHXXfPSYCVzUw3Bc8" class="wolai-block wolai-text"><div><span class="inline-wrap">可视化损失值，需要使用<span class="jill"></span>add_scalar<span class="jill"></span>函数，这里利用一层全连接神经网络，训练一元二次函数的参数。</span></div></div><code-block id="avowxJ8dof2j3YRYKRQpAB" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor
writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span>log_dir<span class="token operator">=</span><span class="token string">'logs'</span><span class="token punctuation">,</span>comment<span class="token operator">=</span><span class="token string">'Linear'</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>
x_train <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
y_train <span class="token operator">=</span> <span class="token number">3</span><span class="token operator">*</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span><span class="token number">2</span><span class="token operator">+</span> <span class="token number">0.2</span><span class="token operator">*</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epoches<span class="token punctuation">)</span><span class="token punctuation">:</span>
  inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span>
  targets <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>y_train<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span>
  output <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
  loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>
  optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
  loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
  optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token comment"># 保存loss的数据与epoch数值</span>
  writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'训练损失值'</span><span class="token punctuation">,</span> loss<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span>
</pre></div></code-block><div id="vXcMWXHf3do8h6tyw3sSHU" class="wolai-block wolai-text"><div></div></div></article><footer></footer></body></html>